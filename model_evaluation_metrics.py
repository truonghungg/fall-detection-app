import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
import pickle
import os
import tensorflow as tf
from tensorflow.keras.models import load_model

print("ƒêang t·∫£i v√† ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ hu·∫•n luy·ªán...")

# Thi·∫øt l·∫≠p style cho bi·ªÉu ƒë·ªì
plt.style.use('default')  # S·ª≠ d·ª•ng style m·∫∑c ƒë·ªãnh
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 12

# ƒê·ªãnh nghƒ©a m√†u s·∫Øc
colors = {
    'accuracy': '#4CAF50',
    'precision': '#2196F3',
    'recall': '#FFC107',
    'f1': '#E91E63',
    'background': '#f8f9fa'
}

# H√†m t·∫°o d·ªØ li·ªáu ƒë√°nh gi√° t·ª´ b√°o c√°o ph√¢n lo·∫°i
def parse_classification_report(report_dict):
    """Chuy·ªÉn ƒë·ªïi classification report dict th√†nh d·∫°ng d·ªÖ v·∫Ω"""
    metrics = {}
    overall_accuracy = report_dict['accuracy']
    
    # Lo·∫°i b·ªè c√°c kh√≥a kh√¥ng ph·∫£i l·ªõp
    activities = [k for k in report_dict.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]
    
    for activity in activities:
        metrics[activity] = {
            'precision': report_dict[activity]['precision'],
            'recall': report_dict[activity]['recall'],
            'f1-score': report_dict[activity]['f1-score'],
            'support': report_dict[activity]['support']
        }
    
    return metrics, overall_accuracy, activities

# Ki·ªÉm tra xem b√°o c√°o ƒë√°nh gi√° c√≥ t·ªìn t·∫°i kh√¥ng
if os.path.exists('models/confusion_matrix.png'):
    print("ƒê√£ t√¨m th·∫•y b√°o c√°o ph√¢n t√≠ch tr∆∞·ªõc ƒë√≥!")

# Ki·ªÉm tra c√°c file m√¥ h√¨nh v√† d·ªØ li·ªáu
model_paths = [f for f in os.listdir() if f.endswith('.h5')]
model_paths.extend([f for f in os.listdir('models') if f.endswith('.h5')] if os.path.exists('models') else [])

if not model_paths:
    print("Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh .h5 n√†o!")
else:
    print(f"T√¨m th·∫•y c√°c file m√¥ h√¨nh: {model_paths}")

data_paths = []
if os.path.exists('preprocessing'):
    data_paths = [f for f in os.listdir('preprocessing') if f.endswith('.pkl')]
    
if not data_paths:
    print("Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu .pkl n√†o trong th∆∞ m·ª•c preprocessing!")
else:
    print(f"T√¨m th·∫•y c√°c file d·ªØ li·ªáu: {data_paths}")

# ∆Øu ti√™n t√¨m ki·∫øm k·∫øt qu·∫£ t·ª´ c√°c lo·∫°i ƒë√£ ƒë∆∞·ª£c ƒë√†o t·∫°o
try:
    # Th·ª≠ t·∫£i d·ªØ li·ªáu tr∆∞·ªõc
    if os.path.exists('preprocessing/processed_data_fixed.pkl'):
        with open('preprocessing/processed_data_fixed.pkl', 'rb') as f:
            X_train, X_test, y_train, y_test, label_encoder = pickle.load(f)
            print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng!")
            print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
            print(f"C√°c l·ªõp: {label_encoder.classes_}")

        # T√¨m m√¥ h√¨nh ƒë·ªÉ t·∫£i
        model_file = None
        if os.path.exists('models/cnn_lstm.h5'):
            model_file = 'models/cnn_lstm.h5'
        elif model_paths:
            model_file = model_paths[0] if os.path.exists(model_paths[0]) else os.path.join('models', model_paths[0])
        
        if model_file and os.path.exists(model_file):
            print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh th√†nh c√¥ng!")
            model = load_model(model_file)
            
            # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
            print("ƒêang d·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra...")
            y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
            
            # T·∫°o classification report
            report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)
            
            # Ph√¢n t√≠ch c√°c ch·ªâ s·ªë
            metrics, overall_accuracy, activities = parse_classification_report(report)
            print(f"\nƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {overall_accuracy:.4f}")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh ƒë·ªÉ t·∫£i!")
            raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω!")
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω")
            
except Exception as e:
    print(f"‚ùå L·ªói: {str(e)}")
    print("S·ª≠ d·ª•ng d·ªØ li·ªáu m·∫´u t·ª´ b√°o c√°o d·ª± √°n...")
    
    # D·ªØ li·ªáu t·ª´ b√°o c√°o d·ª± √°n
    activities = ['falling', 'jogging', 'jumping', 'sitting', 'standing', 'walking']
    
    # D·ªØ li·ªáu t·ª´ k·∫øt qu·∫£ th·ª±c t·∫ø c·ªßa m√¥ h√¨nh
    metrics = {
        'falling': {'precision': 0.98, 'recall': 0.86, 'f1-score': 0.91, 'support': 155},
        'jogging': {'precision': 0.98, 'recall': 0.99, 'f1-score': 0.98, 'support': 155},
        'jumping': {'precision': 1.00, 'recall': 0.98, 'f1-score': 0.99, 'support': 155},
        'sitting': {'precision': 0.86, 'recall': 0.85, 'f1-score': 0.86, 'support': 156},
        'standing': {'precision': 0.86, 'recall': 1.00, 'f1-score': 0.92, 'support': 155},
        'walking': {'precision': 0.98, 'recall': 0.96, 'f1-score': 0.97, 'support': 155}
    }
    
    # Accuracy t·ªïng th·ªÉ
    overall_accuracy = 0.94

# In th√¥ng tin v·ªÅ metrics
print("\n===== METRICS =====")
print(f"ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {overall_accuracy:.4f}")
print(f"C√°c l·ªõp ho·∫°t ƒë·ªông: {activities}")
for activity, metric in metrics.items():
    print(f"{activity}: Precision={metric['precision']:.4f}, Recall={metric['recall']:.4f}, F1={metric['f1-score']:.4f}")

# 1. Bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ch·ªâ s·ªë cho m·ªói ho·∫°t ƒë·ªông
print("\nV·∫Ω bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng ch·ªâ s·ªë...")
plt.figure(figsize=(14, 12))

# T·∫°o DataFrame t·ª´ metrics ƒë·ªÉ d·ªÖ v·∫Ω
df_metrics = pd.DataFrame.from_dict({
    activity: {
        'Precision': metrics[activity]['precision'],
        'Recall': metrics[activity]['recall'],
        'F1-score': metrics[activity]['f1-score']
    } for activity in activities
}).T

# 1. Bi·ªÉu ƒë·ªì c·ªôt nh√≥m
plt.subplot(2, 1, 1)
df_metrics.plot(kind='bar', ax=plt.gca(), width=0.8, alpha=0.8)
plt.xlabel('Ho·∫°t ƒë·ªông', fontsize=12)
plt.ylabel('Gi√° tr·ªã ch·ªâ s·ªë', fontsize=12)
plt.xticks(rotation=30, ha='right')
plt.grid(True, alpha=0.3, axis='y')
plt.ylim(0.8, 1.05)  # TƒÉng gi·ªõi h·∫°n tr√™n ƒë·ªÉ c√≥ ch·ªó cho text

# Th√™m text v·ªõi gi√° tr·ªã accuracy t·ªïng th·ªÉ
plt.text(0.02, 0.95, f'ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {overall_accuracy:.4f}', 
         transform=plt.gca().transAxes, fontsize=10, fontweight='bold',
         bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.5'))

# Th√™m text gi√° tr·ªã l√™n m·ªói c·ªôt
for i, activity in enumerate(activities):
    for j, metric in enumerate(['Precision', 'Recall', 'F1-score']):
        value = df_metrics.loc[activity, metric]
        plt.text(i, value + 0.01, f'{value:.3f}', 
                ha='center', va='bottom', fontsize=8, rotation=90)

plt.legend(title='Ch·ªâ s·ªë', fontsize=10, title_fontsize=10)

# 2. Radar Chart ƒë·ªÉ so s√°nh c√°c ch·ªâ s·ªë gi·ªØa c√°c ho·∫°t ƒë·ªông
print("V·∫Ω radar chart...")
plt.subplot(2, 1, 2)

# T·∫°o d·ªØ li·ªáu cho radar chart
categories = ['Precision', 'Recall', 'F1-score']
N = len(categories)

# G√≥c c·ªßa t·ª´ng tr·ª•c
angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]  # ƒê√≥ng h√¨nh tr√≤n

# Thi·∫øt l·∫≠p tr·ª•c
ax = plt.subplot(2, 1, 2, polar=True)

# V·∫Ω cho t·ª´ng ho·∫°t ƒë·ªông
for i, activity in enumerate(activities):
    values = [metrics[activity]['precision'], metrics[activity]['recall'], 
              metrics[activity]['f1-score']]
    values += values[:1]  # ƒê√≥ng h√¨nh tr√≤n
    
    ax.plot(angles, values, linewidth=2, linestyle='solid', label=activity)
    ax.fill(angles, values, alpha=0.1)

# Thi·∫øt l·∫≠p radar chart
ax.set_theta_offset(np.pi / 2)
ax.set_theta_direction(-1)
ax.set_thetagrids(np.degrees(angles[:-1]), categories)

# Hi·ªÉn th·ªã ƒë∆∞·ªùng k√≠nh
for angle, metric in zip(angles[:-1], categories):
    ax.plot([angle, angle], [0.8, 1.0], linewidth=1, linestyle='-', color='gray', alpha=0.3)

ax.set_ylim(0.8, 1.0)
ax.set_yticks(np.arange(0.8, 1.01, 0.05))
ax.set_yticklabels([f'{x:.2f}' for x in np.arange(0.8, 1.01, 0.05)], fontsize=8)
plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

plt.title('Radar Chart c√°c ch·ªâ s·ªë ƒë√°nh gi√°', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('model_metrics_combined.png', dpi=300, bbox_inches='tight')

# 3. Bi·ªÉu ƒë·ªì nhi·ªát (Heatmap) cho c√°c ch·ªâ s·ªë ƒë√°nh gi√°
print("V·∫Ω heatmap...")
plt.figure(figsize=(14, 8))
heatmap_data = pd.DataFrame({
    'Precision': [metrics[activity]['precision'] for activity in activities],
    'Recall': [metrics[activity]['recall'] for activity in activities],
    'F1-score': [metrics[activity]['f1-score'] for activity in activities]
}, index=activities)

# T·∫°o heatmap
sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.4f', linewidths=.5, vmin=0.8, vmax=1.0)
plt.title('Heatmap c√°c ch·ªâ s·ªë ƒë√°nh gi√° cho t·ª´ng ho·∫°t ƒë·ªông', fontsize=16, fontweight='bold')
plt.xlabel('Ch·ªâ s·ªë ƒë√°nh gi√°', fontsize=14)
plt.ylabel('Ho·∫°t ƒë·ªông', fontsize=14)
plt.tight_layout()
plt.savefig('model_metrics_heatmap.png', dpi=300, bbox_inches='tight')

# 4. Bi·ªÉu ƒë·ªì so s√°nh F1-score gi·ªØa c√°c l·ªõp
print("V·∫Ω bi·ªÉu ƒë·ªì F1-score...")
plt.figure(figsize=(14, 6))
f1_scores = [metrics[activity]['f1-score'] for activity in activities]
support = [metrics[activity]['support'] for activity in activities]

bars = plt.bar(activities, f1_scores, alpha=0.8, color=[plt.cm.tab10(i) for i in range(len(activities))])

# Th√™m text gi√° tr·ªã v√† support l√™n m·ªói c·ªôt
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}\n(n={support[i]})',
             ha='center', va='bottom', fontsize=10)

plt.xlabel('Ho·∫°t ƒë·ªông', fontsize=14)
plt.ylabel('F1-score', fontsize=14)
plt.grid(True, alpha=0.3, axis='y')
plt.ylim(0.8, 1.0)  # ƒê·ªÉ t·∫≠p trung v√†o s·ª± kh√°c bi·ªát
plt.xticks(rotation=30, ha='right')
plt.tight_layout()
plt.savefig('model_f1_comparison.png', dpi=300, bbox_inches='tight')

print("\n‚úÖ ƒê√£ ho√†n th√†nh v√† l∆∞u c√°c bi·ªÉu ƒë·ªì:")
print("üìä model_metrics_combined.png - Bi·ªÉu ƒë·ªì c·ªôt v√† radar chart")
print("üìä model_metrics_heatmap.png - Heatmap c√°c ch·ªâ s·ªë")
print("üìä model_f1_comparison.png - So s√°nh F1-score")

plt.show() 